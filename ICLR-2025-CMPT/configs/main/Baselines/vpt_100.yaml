dataset:
#  name: [caltech101, cifar, dtd, oxford_flowers102, oxford_iiit_pet, sun397, svhn,
#         patch_camelyon, resisc45, eurosat, diabetic_retinopathy,
#         dmlab, kitti, smallnorb_azi, smallnorb_ele, dsprites_loc, dsprites_ori, clevr_dist, clevr_count]
  name: [caltech101, oxford_flowers102, oxford_iiit_pet, sun397,
         patch_camelyon, eurosat, diabetic_retinopathy]
  batch_size: 64

train:
  epochs: 100
#  base_lr: [10, 10, 5, 2.5, 2.5, 50, 0.5,
#            0.5, 2.5, 25, 0.1,
#            500, 250, 10, 50, 0.5, 0.5, 500, 500]
  base_lr: [10, 2.5, 2.5, 10,
            0.5, 5, 2.5]
  modify_lr: True
  lr_decay: True
#  weight_decay: [0.001, 0.001, 0.001, 0.001, 0.001, 0.0001, 0.01,
#                 0.01, 0.001, 0.0001, 0.01,
#                 0, 0, 0.001, 0, 0.01, 0.01, 0, 0]
  weight_decay: [0.001, 0.001, 0.001, 0.001,
                 0.01, 0.0001, 0.01]
  optimizer_type: sgd
  save_prompt: VPT_100

model:
  trainable_para: ['source prompt', 'opt']

  # Source prompt
  src_len: 100
  src_task: 'xavier'
  src_model: roberta  # Useless
  src_proj: 'none'  # Useless

  # Target prompt (there is no target prompt, all the parameters down here are just to make sure target prompts do not exist)
  tgt_len: 0
  tgt_init: xavier
  tgt_proj: none
  tgt_involvement: concat
  cls_pos: after prompt
