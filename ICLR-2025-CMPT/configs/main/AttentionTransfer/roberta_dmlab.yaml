dataset:
  name: dmlab  # !!!
  batch_size: 64  # !!!

train:
  epochs: 100
  base_lr: 0.005  # !!!
  modify_lr: False  # !!!
  lr_decay: True  # !!!
  weight_decay: 0.01  # !!!
  optimizer_type: adam  # !!!
  save_prompt: False

model:
  trainable_para: ['target prompt', 'source projector', 'opt']

  # Source prompt
  src_len: 100
  src_tasks: ['qqp']
  src_model: roberta
  src_projs: ['attention-80']

  # Target prompt
  tgt_len: c_source  # 0 or c_source
  tgt_init: xavier
  tgt_proj: none
  tgt_involvement: concat
  cls_pos: after prompt

log:
  dir_lvl_1: AttentionTransfer-New
  use_tensorboard: False