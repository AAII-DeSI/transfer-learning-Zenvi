train:
  epochs: 100
  base_lr: 0.005
  modify_lr: False
  lr_decay: False
  weight_decay: 0.001
  optimizer_type: adam
  save_prompt: False

model:

  trainable_para: ['target prompt', 'source projector', 'opt']

  # Source prompt
  src_len: 100  # This determines your [CLS] index, so stay 100 if you have 100 prompts prepended
  src_model: roberta
  src_projs: [ 'attention-100', 'attention-90', 'attention-80', 'attention-70', 'attention-60', 'attention-50', 'attention-40', 'attention-30', 'attention-20', 'attention-10' ]

  # Target prompt
  tgt_len: c_source
  tgt_init: xavier
  tgt_proj: none
  tgt_involvement: concat
  cls_pos: after prompt